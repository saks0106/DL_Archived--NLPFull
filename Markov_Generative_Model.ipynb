{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmhZkFxA8FJVfbgvAHGLTM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saks0106/DL_Frequent_Lookout/blob/main/Markov_Generative_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oDQF2b4Uod-Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial = {} # start of a phrase\n",
        "first_order = {} # second word only\n",
        "second_order = {} #when current word depends on prev words aswell"
      ],
      "metadata": {
        "id": "WUkfKkPnqLrC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(s):\n",
        "    return s.translate(str.maketrans('','',string.punctuation))"
      ],
      "metadata": {
        "id": "ft_FD5nEqNGT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAhyIKFQqPYv",
        "outputId": "2675f5c9-f444-411e-988e-67c5c552062e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-10 03:37:20--  https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 56286 (55K) [text/plain]\n",
            "Saving to: ‘robert_frost.txt’\n",
            "\n",
            "\rrobert_frost.txt      0%[                    ]       0  --.-KB/s               \rrobert_frost.txt    100%[===================>]  54.97K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-10-10 03:37:20 (3.63 MB/s) - ‘robert_frost.txt’ saved [56286/56286]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add2dict(d, k, v):\n",
        "  if k not in d:\n",
        "    d[k] = []\n",
        "  d[k].append(v)\n",
        "\n",
        "# [cat, cat, dog, dog, dog, dog, dog, mouse, ...]"
      ],
      "metadata": {
        "id": "eAOPQvNSqRWh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in open('robert_frost.txt'):\n",
        "  tokens = remove_punctuation(line.rstrip().lower()).split()\n",
        "\n",
        "  T = len(tokens)\n",
        "  for i in range(T):\n",
        "    t = tokens[i]\n",
        "    if i == 0: #First word of the sentence\n",
        "      # measure the distribution of the first word\n",
        "      initial[t] = initial.get(t, 0.) + 1 #counting the words ie repeation\n",
        "    else:\n",
        "      t_1 = tokens[i-1] #Prev word\n",
        "      if i == T - 1: #If end of sentence\n",
        "        # measure probability of ending the line\n",
        "        add2dict(second_order, (t_1, t), 'END') # {second_order}, (prev_word, current_word), 'END'\n",
        "      if i == 1: #Second word in the sentence\n",
        "        # measure distribution of second word\n",
        "        # given only first word\n",
        "        add2dict(first_order, t_1, t)\n",
        "      else:\n",
        "        t_2 = tokens[i-2]\n",
        "        add2dict(second_order, (t_2, t_1), t)"
      ],
      "metadata": {
        "id": "V7Fcfs3zqUGc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the distributions as initial dict as it contains actual counts\n",
        "initial_total = sum(initial.values())\n",
        "for t, c in initial.items():\n",
        "    initial[t] = c / initial_total"
      ],
      "metadata": {
        "id": "jbhdYiZLtRQy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert [cat, cat, cat, dog, dog, dog, dog, mouse, ...]\n",
        "# into {cat: 0.5, dog: 0.4, mouse: 0.1}\n",
        "\n",
        "def list2pdict(ts):\n",
        "  # turn each list of possibilities into a dictionary of probabilities\n",
        "  d = {}\n",
        "  n = len(ts)\n",
        "  for t in ts:\n",
        "    d[t] = d.get(t, 0.) + 1\n",
        "  for t, c in d.items():\n",
        "    d[t] = c / n\n",
        "  return d"
      ],
      "metadata": {
        "id": "_8Cjf9XotTB6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t_1, ts in first_order.items():\n",
        "  # replace list with dictionary of probabilities\n",
        "  first_order[t_1] = list2pdict(ts)"
      ],
      "metadata": {
        "id": "dBlFqMd9tVT1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, ts in second_order.items():\n",
        "  second_order[k] = list2pdict(ts) #tuple of 2 words"
      ],
      "metadata": {
        "id": "EzgL_CUDtW_L"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_word(d):\n",
        "  # print \"d:\", d\n",
        "  p0 = np.random.random() #0-1\n",
        "  # print \"p0:\", p0\n",
        "  cumulative = 0\n",
        "  for t, p in d.items():\n",
        "    cumulative += p\n",
        "    if p0 < cumulative:\n",
        "      return t\n",
        "  assert(False) # should never get here"
      ],
      "metadata": {
        "id": "fHT-Sgf6tYmB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate():\n",
        "  for i in range(4): # generate 4 lines\n",
        "    sentence = []\n",
        "\n",
        "    # initial word\n",
        "    w0 = sample_word(initial)\n",
        "    sentence.append(w0)\n",
        "\n",
        "    # sample second word\n",
        "    w1 = sample_word(first_order[w0])\n",
        "    sentence.append(w1)\n",
        "\n",
        "    # second-order transitions until END\n",
        "    while True:\n",
        "      w2 = sample_word(second_order[(w0, w1)])\n",
        "      if w2 == 'END':\n",
        "        break\n",
        "      sentence.append(w2)\n",
        "      w0 = w1\n",
        "      w1 = w2\n",
        "    print(' '.join(sentence))"
      ],
      "metadata": {
        "id": "MQFKb_lXta08"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF2AZjD3u6rV",
        "outputId": "fb38c5bc-9b1e-40e1-b012-bb0720efaee3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i heard him downstairs in the huse business\n",
            "might just as empty\n",
            "but rain spoiled all the worse\n",
            "with all the money goes so fast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise:\n",
        "#\n",
        "# Determine the vocabulary size (V)\n",
        "# We know that pi has shape V, A1 has shape V x V, and A2 has shape V x V x V\n",
        "#\n",
        "# In comparison, how many values are stored in our dictionaries?\n",
        "\n",
        "\n",
        "# Exercise 2:\n",
        "# We can skip the step where we accumulate all the possible next words in a list\n",
        "# E.g. [cat, cat, dog, dog, dog, ...]\n",
        "#\n",
        "# Instead, like we do with the initial state distribution, create the dictionary\n",
        "# of counts directly as you loop through the data.\n",
        "#\n",
        "# You'll no longer need list2pdict()"
      ],
      "metadata": {
        "id": "GQYVDziJu8g3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}