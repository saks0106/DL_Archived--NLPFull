{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTm2kYVnDJUBt86zIz2yAN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saks0106/DL_Frequent_Lookout/blob/main/POS_Tagging_M2M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBrOWZvncqES",
        "outputId": "47fe3381-66fc-46cd-d71d-3475cbb4b7b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = brown.tagged_sents(tagset='universal')\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq7AX8Bpcv_w",
        "outputId": "4be064e3-5123-4477-99b2-1f6e3b037215"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets = []\n",
        "inputs = []\n",
        "\n",
        "for sentence_tag_pairs in corpus:\n",
        "  tokens = []\n",
        "  target = []\n",
        "  for token, tag in sentence_tag_pairs:\n",
        "    tokens.append(token)\n",
        "    target.append(tag)\n",
        "  inputs.append(tokens)\n",
        "  targets.append(target)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, Bidirectional\n",
        "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ],
      "metadata": {
        "id": "222Bl1KGc2hF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs, test_inputs, train_targets, test_targets = train_test_split(\n",
        "    inputs,\n",
        "    targets,\n",
        "    test_size=0.3,\n",
        ")\n",
        "# Convert sentences to sequences\n",
        "\n",
        "MAX_VOCAB_SIZE = None\n",
        "\n",
        "# capitalization might be useful - test it\n",
        "should_lowercase = False\n",
        "word_tokenizer = Tokenizer(\n",
        "    num_words=MAX_VOCAB_SIZE,\n",
        "    lower=should_lowercase,\n",
        "    oov_token='UNK', #out of vocab token if not found in vocab, important\n",
        ")\n",
        "# otherwise unknown tokens will be removed and len(input) != len(target)\n",
        "# input words and target words will not be aligned!\n",
        "\n",
        "word_tokenizer.fit_on_texts(train_inputs)\n",
        "train_inputs_int = word_tokenizer.texts_to_sequences(train_inputs)\n",
        "test_inputs_int = word_tokenizer.texts_to_sequences(test_inputs)"
      ],
      "metadata": {
        "id": "Dls4gXlnc6R3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get word -> integer mapping\n",
        "word2idx = word_tokenizer.word_index\n",
        "V = len(word2idx)\n",
        "print('Found %s unique tokens.' % V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rejqDlp5dCC0",
        "outputId": "9d6d83da-5899-47d2-d176-87fa7f68e92b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 47464 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/11264684/flatten-list-of-lists\n",
        "def flatten(list_of_lists):\n",
        "  flattened = [val for sublist in list_of_lists for val in sublist]\n",
        "  return flattened"
      ],
      "metadata": {
        "id": "LAf4QL_udEIV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_targets = set(flatten(train_targets))\n",
        "all_train_targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPi19oGOdGJU",
        "outputId": "856908e5-dddb-4cbb-a5e5-92c0c28fbeba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.',\n",
              " 'ADJ',\n",
              " 'ADP',\n",
              " 'ADV',\n",
              " 'CONJ',\n",
              " 'DET',\n",
              " 'NOUN',\n",
              " 'NUM',\n",
              " 'PRON',\n",
              " 'PRT',\n",
              " 'VERB',\n",
              " 'X'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_test_targets = set(flatten(test_targets))\n",
        "all_test_targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35Kxd3ahdHnV",
        "outputId": "2f931a7c-62dd-4b9e-e6cc-f85317fb1cd6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.',\n",
              " 'ADJ',\n",
              " 'ADP',\n",
              " 'ADV',\n",
              " 'CONJ',\n",
              " 'DET',\n",
              " 'NOUN',\n",
              " 'NUM',\n",
              " 'PRON',\n",
              " 'PRT',\n",
              " 'VERB',\n",
              " 'X'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_targets == all_test_targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vItjzIASdJOT",
        "outputId": "4b9ecb66-e7eb-4b49-933c-fbce40137d62"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert targets to sequences\n",
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_targets)\n",
        "train_targets_int = tag_tokenizer.texts_to_sequences(train_targets)\n",
        "test_targets_int = tag_tokenizer.texts_to_sequences(test_targets)\n",
        "\n",
        "# save for later\n",
        "train_targets_int_unpadded = train_targets_int\n",
        "test_targets_int_unpadded = test_targets_int\n",
        "# before padding, find max document length\n",
        "# because we don't want to truncate any inputs\n",
        "# which would also truncate targets\n",
        "maxlen_train = max(len(sent) for sent in train_inputs) #finding max len for training\n",
        "maxlen_test = max(len(sent) for sent in test_inputs)\n",
        "T = max((maxlen_train, maxlen_test))"
      ],
      "metadata": {
        "id": "SlAK4IdldL-a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad sequences so that we get a N x T matrix\n",
        "train_inputs_int = pad_sequences(train_inputs_int, maxlen=T)\n",
        "print('Shape of data train tensor:', train_inputs_int.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b97B0itIdPJl",
        "outputId": "4f5f7fbd-9229-4e0c-d8d1-48ec0b78fb5d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data train tensor: (40138, 180)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs_int = pad_sequences(test_inputs_int, maxlen=T)\n",
        "print('Shape of data test tensor:', test_inputs_int.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of7uqObidRTO",
        "outputId": "2a7dd439-5414-4dbd-dcf7-d0e1d13dde81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data test tensor: (17202, 180)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_targets_int = pad_sequences(train_targets_int, maxlen=T)\n",
        "print('Shape of train targets tensor:', train_targets_int.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxvxlCGqdSqP",
        "outputId": "42ad0de1-36bc-49cc-d361-7ac3ca4924da"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train targets tensor: (40138, 180)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_targets_int = pad_sequences(test_targets_int, maxlen=T)\n",
        "print('Shape of test targets tensor:', test_targets_int.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hBzScEOdUtO",
        "outputId": "61eaa109-97d9-49e9-b27b-39bb75e59255"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of test targets tensor: (17202, 180)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of classes\n",
        "K = len(tag_tokenizer.word_index) + 1\n",
        "K"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tKBzuo-dWRf",
        "outputId": "376705f7-c2ba-422f-b6d9-745ecb6b71bc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "\n",
        "# We get to choose embedding dimensionality\n",
        "D = 32\n",
        "\n",
        "i = Input(shape=(T,))\n",
        "# mask_zero=True way slower on GPU than CPU!\n",
        "x = Embedding(V + 1, D, mask_zero=True)(i)\n",
        "#we have padded input and target, so mask_zero ie input zero will be ignored\n",
        "x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
        "#BRNN to capture prev and next words\n",
        "x = Dense(K)(x)\n",
        "\n",
        "model = Model(i, x)"
      ],
      "metadata": {
        "id": "kzstR1CadXwA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and fit\n",
        "model.compile(\n",
        "  loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# NOTE: you \"could\" speed this up by creating a custom loss, since Tensorflow's\n",
        "# implementation of mask_zero currently sucks, but it's sufficiently advanced\n",
        "# to be outside the scope of this course\n",
        "# In my experiments, CPU is faster than GPU in all cases, and CPU for custom\n",
        "# loss is faster than CPU for mask_zero\n",
        "\n",
        "# > 300-400s per epoch on CPU\n",
        "# > 30 MINUTES per epoch on GPU\n",
        "print('Training model...')\n",
        "r = model.fit(\n",
        "  train_inputs_int,\n",
        "  train_targets_int,\n",
        "  epochs=5,\n",
        "  validation_data=(test_inputs_int, test_targets_int))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBqjYYnWdZt3",
        "outputId": "49887cd4-7a1b-497f-97aa-993542bb7e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Epoch 1/5\n",
            "1255/1255 [==============================] - 369s 278ms/step - loss: 0.4639 - accuracy: 0.8582 - val_loss: 0.1080 - val_accuracy: 0.9667\n",
            "Epoch 2/5\n",
            "1255/1255 [==============================] - 308s 246ms/step - loss: 0.0708 - accuracy: 0.9782 - val_loss: 0.0853 - val_accuracy: 0.9720\n",
            "Epoch 3/5\n",
            "1255/1255 [==============================] - 295s 235ms/step - loss: 0.0436 - accuracy: 0.9862 - val_loss: 0.0832 - val_accuracy: 0.9735\n",
            "Epoch 4/5\n",
            "1255/1255 [==============================] - 328s 261ms/step - loss: 0.0323 - accuracy: 0.9899 - val_loss: 0.0847 - val_accuracy: 0.9739\n",
            "Epoch 5/5\n",
            " 293/1255 [======>.......................] - ETA: 3:21 - loss: 0.0256 - accuracy: 0.9924"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss per iteration\n",
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "ynELUwvUdbnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy per iteration\n",
        "plt.plot(r.history['accuracy'], label='train acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val acc')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "heKYXzncdf4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# True model accuracy - above includes padding\n",
        "\n",
        "# first get length of each sequence\n",
        "train_lengths = []\n",
        "for sentence in train_inputs:\n",
        "  train_lengths.append(len(sentence))\n",
        "\n",
        "test_lengths = []\n",
        "for sentence in test_inputs:\n",
        "  test_lengths.append(len(sentence))"
      ],
      "metadata": {
        "id": "R-AnqwybdhfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_probs = model.predict(train_inputs_int) # N x T x K\n",
        "train_predictions = []\n",
        "for probs, length in zip(train_probs, train_lengths):\n",
        "  # probs is T x K\n",
        "  probs_ = probs[-length:]\n",
        "  preds = np.argmax(probs_, axis=1)\n",
        "  train_predictions.append(preds)\n",
        "\n",
        "# flatten\n",
        "flat_train_predictions = flatten(train_predictions)\n",
        "flat_train_targets = flatten(train_targets_int_unpadded)\n",
        "test_probs = model.predict(test_inputs_int) # N x T x K\n",
        "test_predictions = []\n",
        "for probs, length in zip(test_probs, test_lengths):\n",
        "  # probs is T x K\n",
        "  probs_ = probs[-length:]\n",
        "  preds = np.argmax(probs_, axis=1)\n",
        "  test_predictions.append(preds)\n",
        "\n",
        "# flatten\n",
        "flat_test_predictions = flatten(test_predictions)\n",
        "flat_test_targets = flatten(test_targets_int_unpadded)"
      ],
      "metadata": {
        "id": "HbBs4ZLGdi7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "print(\"Train acc:\", accuracy_score(flat_train_targets, flat_train_predictions))\n",
        "print(\"Test acc:\", accuracy_score(flat_test_targets, flat_test_predictions))\n",
        "\n",
        "print(\"Train f1:\",\n",
        "      f1_score(flat_train_targets, flat_train_predictions, average='macro'))\n",
        "print(\"Test f1:\",\n",
        "      f1_score(flat_test_targets, flat_test_predictions, average='macro'))"
      ],
      "metadata": {
        "id": "prVOnZQfdlYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Baseline model: map word to tag\n",
        "from collections import Counter\n",
        "\n",
        "# https://stackoverflow.com/questions/1518522/find-the-most-common-element-in-a-list\n",
        "def most_common(lst):\n",
        "  data = Counter(lst)\n",
        "  return data.most_common(1)[0][0]"
      ],
      "metadata": {
        "id": "U5xIvtPUdnC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token2tags = {k: [] for k, v in word2idx.items()}\n",
        "\n",
        "# remove UNK token\n",
        "del token2tags['UNK']\n",
        "\n",
        "for tokens, tags in zip(train_inputs, train_targets):\n",
        "  for token, tag in zip(tokens, tags):\n",
        "    if should_lowercase:\n",
        "      token = token.lower()\n",
        "    if token in token2tags:\n",
        "      token2tags[token].append(tag)\n",
        "\n",
        "token2tag = {k: most_common(v) for k, v in token2tags.items()}"
      ],
      "metadata": {
        "id": "oupOLgmDdrK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute accuracy\n",
        "\n",
        "train_predictions = []\n",
        "for sentence in train_inputs:\n",
        "  predictions = []\n",
        "  for token in sentence:\n",
        "    if should_lowercase:\n",
        "      token = token.lower()\n",
        "    predicted_tag = token2tag[token]\n",
        "    predictions.append(predicted_tag)\n",
        "  train_predictions.append(predictions)\n",
        "flat_train_predictions = flatten(train_predictions)\n",
        "flat_train_targets = flatten(train_targets)"
      ],
      "metadata": {
        "id": "KA1rK04FdsrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = []\n",
        "for sentence in test_inputs:\n",
        "  predictions = []\n",
        "  for token in sentence:\n",
        "    predicted_tag = token2tag.get(token, 'INCORRECT')\n",
        "    predictions.append(predicted_tag)\n",
        "  test_predictions.append(predictions)\n",
        "flat_test_predictions = flatten(test_predictions)\n",
        "flat_test_targets = flatten(test_targets)"
      ],
      "metadata": {
        "id": "NU30XlwNduHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train acc:\", accuracy_score(flat_train_targets, flat_train_predictions))\n",
        "print(\"Test acc:\", accuracy_score(flat_test_targets, flat_test_predictions))\n",
        "\n",
        "print(\"Train f1:\",\n",
        "      f1_score(flat_train_targets, flat_train_predictions, average='macro'))\n",
        "print(\"Test f1:\",\n",
        "      f1_score(flat_test_targets, flat_test_predictions, average='macro'))"
      ],
      "metadata": {
        "id": "g15KxqNIdvVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise #1: use different layers (GRU, SimpleRNN), different number of hidden\n",
        "# units, number of layers, etc. as before\n",
        "# Also, repeat the exercise with CNNs instead of RNNs\n",
        "# Exercise #2: repeat the same code, but include the 2 mistakes\n",
        "# Mistake #1: not having an OOV token (target len != input len, misalignment)\n",
        "#     Also, verify that target misalignment exists\n",
        "# Mistake #2: not ignoring padding in loss computation\n",
        "# Observe that you get 99% accuracy, even with a bad model.\n",
        "# Exercise: why is the accuracy so high? (discuss on Q&A with your peers)\n",
        "\n",
        "# I've seen these mistakes on Kaggle and on blogs (you know how I like\n",
        "# to read a.k.a. judge bad DS/ML bloggers and marketers). The mistake arises\n",
        "# from \"trusting the library too much\", assuming you can just use the\n",
        "# Tokenizer() with default values and Embedding() with default values, and not\n",
        "# thinking about what they actually do.\n",
        "\n",
        "# i.e. it's important to THINK, not just \"use libraries\"."
      ],
      "metadata": {
        "id": "Z8zz0zz0dxD1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}